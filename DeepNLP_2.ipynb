{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepNLP-2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPoVkgSWnA+UEmQk6gf4SlT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdheram/DeepNLP/blob/master/DeepNLP_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w6b3w7KAK2d"
      },
      "source": [
        "# Recurrent Neural network \n",
        "\n",
        "Source : http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8znwSIxvli0"
      },
      "source": [
        "**What is RNN**?\n",
        "\n",
        "* CNN is pretty basic where the output is dependent on the input provided.\n",
        "* When dealing with text sequences (time related dependence), the information from just the previous output might not be sufficient. Text sequences need information from past inputs.\n",
        "\n",
        "* Language modelling task is a task similar where the problem is to predict the next word in a sequence based on previous sequence of words.\n",
        "* In other words, language model is a task of assigning probabilty distribution to sequence of words.\n",
        "  1. N-gram language model : Based on n-grams (n consecutive words occuring side by side) the probability of n-grams are calculated from the corpus. The probabilities are calculated based on the counting  and on markov assumuption that prob(n) depends on prob(n-1)). \n",
        "    * Problems:\n",
        "      * **Sparsity problem** where the n-gram probability might be zero in some cases of sequences.\n",
        "      * **Storing** n-gram probabilities.\n",
        "  2. Window-based Neural Language Model : Neural Probabilistic Language Model. The model learns a distributed representation of each word along the window *(word embeddings - concatenated words/one-hot vectors)* and the probability function for *word sequences*. \n",
        "    * Problems:\n",
        "      * **Fixed window** problem\n",
        "      * Different weights used for different words which leads to **No symmetry** in how the inputs are processed.\n",
        "* To handle these problems, RNN architecture has been used where the Recurrent Neural Networks (RNN) are capable of\n",
        "conditioning the model on **all** previous words in the corpus.\n",
        "* RNN architecture consists of recurrent unit (hidden layer / Memory unit) at each time step which holds a number of neurons and tells \"which memories\" to pay attention to when making new memory.\n",
        "* This hidden layer performs a linear matrix operation followed by a non-linear operation (e.g. tanh())\n",
        "* At each time-step, there are two inputs to hidden layer: \n",
        "\n",
        "    1.The output of the previous layer h(t-1)\n",
        "\n",
        "    2.The input at that timestep x(t). \n",
        "    \n",
        "    Since, time is the independent variable, the data variable need to be indexed by time. \n",
        "\n",
        "    ```x(t) - input; \n",
        "    h(t) - hidden unit dependent on input x(t) and h(t-1) output of the non-linear function at the previous time-step(t-1) ```\n",
        "* The **output of previous hidden layer h(t-1)** is multiplied by a weight matrix **W(hh)**; while the **input x(t)** is multiplied by **w(hx)** to produce output features h(t).\n",
        "* The **h(t)** produced is **multiplied** with weight matrix **W(s)** and run through softmax over vocab to obtain a prediction **output y(t)** of the next word. \n",
        "\n",
        "RNN Pipeline:\n",
        "\n",
        "      Big picture : x(t) + h(t-1) -> h(t) -> y(t)\n",
        "\n",
        "      Recurrent unit h(t) details : \n",
        "      h(t) = sigmoid_ActFunc [W(hh)*h(t-1) + W(hx)*x(t)]\n",
        "\n",
        "      Output Probability distribution over vocab :\n",
        "      y(t) = softmax[W(s)*h(t)]\n",
        "\n",
        "      Shape(W(input)) = shape(x(t)) * shape(h(t))\n",
        "      Shape(W(hidden)) = shape(h(t)) * shape(h(t))\n",
        "      Shape(W(output)) = shape(h(t)) * shape(y(t))\n",
        "\n",
        "* **Same weights W(hh) and W(hx) are applied repeatedly at each timestep.** Hence,\n",
        "  1. The number of parameters\n",
        "the model has to learn is less.\n",
        "  2. The number of parameters is independent of the input sequence, thus defeating the curse of dimentionality!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpSi0LBXzMTi"
      },
      "source": [
        "**Why RNN architecture**?\n",
        "\n",
        "* A feed forward neural network can represent/approximate any type of architecture.\n",
        "*  The goal of a RNN implementation is to enable propagating context information through faraway time-steps.\n",
        "\n",
        "RNN architecture vs feed forward neural network architecture when dealing with sequences.\n",
        "\n",
        "```\n",
        "Eg. \n",
        "T = 20 (sequence length)\n",
        "D = 10 (input dimentionality/size)\n",
        "M = 15 (hidden layer size)\n",
        "K = 3 (number of output classes)\n",
        "```\n",
        "\n",
        "Feedforward:\n",
        "\n",
        "```\n",
        "Input-to-hidden weight matrix size = TxDxTxM = 60,000 weights in total \n",
        "Hidden-to-output weight matrix size = TxMxMxK = 18,000 weights in total \n",
        "Total : 78,000 parameters (Weights updates) \n",
        "```\n",
        "\n",
        "RNN :\n",
        "\n",
        "```\n",
        "Input-to-hidden weight matrix size = DxM = 150 weights in total \n",
        "Hidden-to-output weiht matrix size = MxM = 225 weights in total\n",
        "Hidden-to-output weight matrix size = MxK = 45 weights in total \n",
        "total = 420 parameters (Weights updates)\n",
        "```\n",
        "Hence, ANN lacks the following when dealing with sequences:\n",
        "\n",
        "1. Due to parameters size.\n",
        "2. Lack of structure. (as everything is connected to everything)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG7NszZCLTkp"
      },
      "source": [
        "**RNN loss function and Perplexity of LM**:\n",
        "\n",
        "Loss function is applied as sum over the entire vocabulary at time-step t. Perplexity of LM is the mearure of confusion and is the 2 to the power of negative log probability of the cross entropy error function.\n",
        "\n",
        "Loss function : Cross entropy (-log loss)\n",
        "\n",
        "perplexity : 2^cross entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSS8zcGRP8zP"
      },
      "source": [
        "**RNN advantages and disadvantages** :\n",
        "\n",
        "Advantages:\n",
        "\n",
        "1. Can process input sequences of any length.\n",
        "2. Model size does not increase for longer input sequence\n",
        "lengths.\n",
        "3. The same weights are applied to every timestep of the input, so\n",
        "there is symmetry in how inputs are processed.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "1. Computation is slow - because it is sequential, it cannot be parallelized.\n",
        "2. In practice, it is difficult to access information from many steps\n",
        "back (long-term dependencies) due to problems like vanishing and exploding gradients.\n",
        "\n",
        "Vanishing and explosion gradient problem :\n",
        "\n",
        "* RNN propagates weight matrices from one-time step to another and when updating the weights during the backpropagation, the gradients values gradually vanishes as they propagate earlier in steps. Hence, for long sentences, the probability of predicting the correct next word decreases.\n",
        "\n",
        "  * Two techniques to handle:\n",
        "    1. Initialize the hidden weight as an identity matrix rather than random.\n",
        "    2. Use Recitified Linear Units (ReLU) instead of sigmoid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmlRbRkWuiTl"
      },
      "source": [
        "## Varients of RNN: GRU & LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiligT-C1LDi"
      },
      "source": [
        "### Gated Recurrent Unit (GRU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0UZRRlh5Wdj"
      },
      "source": [
        "Why?\n",
        "\n",
        "* To better handle long-term dependencies than above Simple Recurrent Unit (SRU).\n",
        "\n",
        "How ?\n",
        "\n",
        "* Introduces gates which allow the recurrent unit to remember or forget values.\n",
        "* Using 4 componenets of GRU\n",
        "  1. **New Memory Generation / Candidate state [˜h(t)]**: A new memory ˜h(t) is the consolidation of\n",
        "a new input word x(t) with the past hidden state h(t−1) based on Reset gate.\n",
        "  2. **Reset gate [r(t)]** : The reset signal r(t) is responsible for determining how\n",
        "important h(t−1) is to the summarization ˜h(t). Values ranges between 0-1.\n",
        "  3. **Update gate [z(t)]** : The update signal z(t) is responsible for determining how much of h(t−1) should be carried forward to the next state. Values ranges between 0-1. if z(t) ≈ 1, then past hidden state h(t−1) is almost entirely copied out to ht.\n",
        "Conversely, if z(t) ≈ 0, then mostly the new memory ˜h(t)\n",
        "is forwarded\n",
        "to the next hidden state.\n",
        "  4. **Hidden / next state [h(t)]** : The hidden state h(t)\n",
        "is finally generated using the\n",
        "past hidden input h(t−1) and the new memory generated ˜h(t) with the\n",
        "advice of the update gate. \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMpKDqgwEBYh"
      },
      "source": [
        "[**Reset gate -> New memory** -> **update gate**] -> **hidden state / next state**   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbysv2aqkwbi"
      },
      "source": [
        "Keras :\n",
        "\n",
        "* To ouput the hidden state at a time step, pass in `return_state = Ture`. \n",
        "\n",
        "Eg.  output, h = GRU(128, return_state = True) (input)\n",
        "\n",
        "**The recurrent layer output and hidden state output in a timestep are the same** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk3LPuKc9Hwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb4764a-79f7-407b-c50e-176f9332ef53"
      },
      "source": [
        "# RNN test\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, GRU\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "T = 8\n",
        "D = 2\n",
        "M = 3\n",
        "\n",
        "X = np.random.randn(1,T,D)\n",
        "\n",
        "def gru():\n",
        "  input_ = Input(shape = (T,D))\n",
        "  rnn = GRU(M, return_state = True)\n",
        "  x = rnn(input_)\n",
        "\n",
        "  model = Model(inputs = input_, outputs =x)\n",
        "  o, h = model.predict(X)\n",
        "  print(\"o:\",o)\n",
        "  print(\"h:\",h)\n",
        "  if(o==h).all():\n",
        "    print(\"Output and hidden state are same\")\n",
        "\n",
        "def gru1():\n",
        "  input_ = Input(shape = (T,D))\n",
        "  rnn = GRU(M, return_state = True, return_sequences=True)\n",
        "  x = rnn(input_)\n",
        "\n",
        "  model = Model(inputs = input_, outputs =x)\n",
        "  o, h = model.predict(X)\n",
        "  print(\"o:\",o)\n",
        "  print(\"h:\",h)\n",
        "  if(o==h).all():\n",
        "    print(\"Output and hidden state are same\")\n",
        "\n",
        "gru()\n",
        "print(\"----------------\")\n",
        "gru1()\n",
        "print(\"Length of output is 8 as length of sequences is 8\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd6ecb93ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "o: [[0.3780474  0.07000626 0.29066744]]\n",
            "h: [[0.3780474  0.07000626 0.29066744]]\n",
            "Output and hidden state are same\n",
            "----------------\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd6f4283b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "o: [[[ 0.13738938  0.23651515 -0.00949176]\n",
            "  [-0.19013974  0.07453187 -0.06851615]\n",
            "  [-0.23519921 -0.0421285  -0.07376076]\n",
            "  [-0.18998587  0.08858846 -0.10147636]\n",
            "  [-0.2643824  -0.09200244 -0.09300735]\n",
            "  [-0.00317092 -0.2182115   0.07165496]\n",
            "  [-0.16735573 -0.02561122 -0.03028229]\n",
            "  [-0.37837565  0.24546923 -0.12733148]]]\n",
            "h: [[-0.37837565  0.24546923 -0.12733148]]\n",
            "Length of o is 8 as return sequences return the last output\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "156TP1jh4nYP"
      },
      "source": [
        "### Long-short Term Memory Unit (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFgAFL207PQR"
      },
      "source": [
        "Why?\n",
        "\n",
        "* To better handle long-term dependencies with more gates than GRU.\n",
        "\n",
        "How ?\n",
        "\n",
        "* Introduces more gates than GRU\n",
        "  1. **New memory cell / candidate cell [c˜(t)]** : Compute new memory based on new input x(t) and hidden state h(t-1).  tanh activation used where values range between -1 to n .\n",
        "  2. **Input gate [i(t)]** : Compute the importance of new input based on input x(t) and hidden state h(t-1). Sigmoid activation used where values range between 0-1.\n",
        "  3. **Forget gate [f(t)]** : Computes the importance of past memory cell c(t-1) based on input x(t) and hidden state h(t-1). Sigmoid activation used where values range between 0-1. \n",
        "  4. **Final memory cell / cell state [c(t)]** :  Takes advice of forget gate f(t) which is based on past memory c(t-1) and input gate i(t) and new memory cell c˜(t) and sums up these two to produce the c(t).\n",
        "  5. **Output/exposure gate / hidden state [h(t)]** : Computes how much of Final memory cell should be exposed.The signal it produces\n",
        "to indicate this is o(t) and this is used to gate the point-wise tanh of\n",
        "the memory c(t)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtY-fWfu8dXi"
      },
      "source": [
        "  **tanh[New memory] + sigmoid[input gate] + sigmoid[forget gate] -> cell state + hidden state**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJDx7XYVlgFf"
      },
      "source": [
        "**Keras** :\n",
        "\n",
        "* To ouput the hidden state at a time step, pass in `return_state = Ture`. \n",
        "\n",
        "Eg.  output, h, c = LSTM(128, return_state = True) (input)\n",
        "\n",
        "* To output the sequence at each time step, pass in `return_sequences = True` or `return_sequences = False` to output the last timestep output.\n",
        "\n",
        "Eg. output, h,c = LSTM(128, return_state = True, return_sequences = True) (input)\n",
        "\n",
        "**The recurrent layer output and hidden state output in a timestep are the same**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkrVxjx04tsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f031929-685c-4dc6-e202-efd469aec47d"
      },
      "source": [
        "# RNN test\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "T = 8\n",
        "D = 2\n",
        "M = 3\n",
        "\n",
        "X = np.random.randn(1,T,D)\n",
        "\n",
        "def lstm():\n",
        "  input_ = Input(shape = (T,D))\n",
        "  rnn = LSTM(M, return_state = True)\n",
        "  x = rnn(input_)\n",
        "\n",
        "  model = Model(inputs = input_, outputs =x)\n",
        "  o, h, c = model.predict(X)\n",
        "  print(\"o:\",o)\n",
        "  print(\"h:\",h)\n",
        "  print(\"c:\",c)\n",
        "  if(o==h).all():\n",
        "    print(\"Output and hidden state are same\")\n",
        "\n",
        "def lstm1():\n",
        "  input_ = Input(shape = (T,D))\n",
        "  rnn = LSTM(M, return_state = True, return_sequences=True)\n",
        "  x = rnn(input_)\n",
        "\n",
        "  model = Model(inputs = input_, outputs =x)\n",
        "  o, h, c = model.predict(X)\n",
        "  print(\"o:\",o)\n",
        "  print(\"h:\",h)\n",
        "  print(\"c:\",c)\n",
        "  if(o==h).all():\n",
        "    print(\"Output and hidden state are same\")\n",
        "\n",
        "lstm()\n",
        "print(\"----------------\")\n",
        "lstm1()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd6f426b3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "o: [[-0.18850638  0.02607018 -0.08591584]]\n",
            "h: [[-0.18850638  0.02607018 -0.08591584]]\n",
            "c: [[-0.40824345  0.07652194 -0.26011327]]\n",
            "Output and hidden state are same\n",
            "----------------\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd6ed621950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "o: [[[-0.12279311 -0.08188009  0.01587407]\n",
            "  [-0.29582298  0.23334044 -0.3461163 ]\n",
            "  [ 0.0368443   0.17599915 -0.3949932 ]\n",
            "  [-0.09588622  0.40716726 -0.17474164]\n",
            "  [-0.07354331  0.13536981 -0.11416193]\n",
            "  [ 0.04567917 -0.01000865 -0.01194016]\n",
            "  [ 0.13145879  0.03977356 -0.02190189]\n",
            "  [ 0.09788015 -0.10855313  0.05099949]]]\n",
            "h: [[ 0.09788015 -0.10855313  0.05099949]]\n",
            "c: [[ 0.31439742 -0.23249981  0.16215065]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYTgDMShmH5G"
      },
      "source": [
        "### RNN tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE_0dp_wnSwq"
      },
      "source": [
        "RNN inputs:\n",
        "  * sequence length x input dimentionality (TxD).\n",
        "\n",
        "  Eg.\n",
        "\n",
        "    Input sequence length (T) : \"The quick brown fox jumps\" : 5 words,\n",
        "    input dimentionality (D): 50 dimentions/word, \n",
        "\n",
        "    Input_shape = 5 x 50\n",
        "\n",
        "RNN outputs:\n",
        "  * Sequence length x Number of classes (TxK)\n",
        "\n",
        "  Eg. \n",
        "\n",
        "    Input sequence length (T) : \"The quick brown fox jumps\" : 5 words,\n",
        "    Output Dimentionality (D) : 5 classes\n",
        "\n",
        "    Output_shape = 5 x 5  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYjA4etCr8Bw"
      },
      "source": [
        "Types:\n",
        "\n",
        "1. Many to one RNN: \n",
        "  * Take input sequence and produce a single output (Based on category or classes).\n",
        "  * Eg. sentiment classification or spam classification\n",
        "  * In Keras, \n",
        "    * Pass `input_sequence = false` to return only the last recurrent unit output.\n",
        "    * Take a global_max_pool of all the recurrent unit which returns the maximum values out of all the recurrent units. \n",
        "\n",
        "      Eg. \"Hi, I am nigerian prince ...\" it is enought to look at the first few words, take gloabal max pool and predict output than taking the output based on the last word.  \n",
        "\n",
        "2. Many to Many RNN:\n",
        "\n",
        "  * Take input sequence of length T and produce T outputs. An output per time step.\n",
        "  * Eg. Parts of speech POS-tagging, Named entity recognition. (Pass `input_sequence = true` to return only the last recurrent unit output.)\n",
        "  * Eg. Machine translation, chatbots, question answering.\n",
        "  * If length of input sequence is not equal to output sequence (Machine traslation English-Japanese)\n",
        "    * Strategy : Pad sequences to equal length.\n",
        "3. One to Many RNN:\n",
        "  * Eg. Poetry generation. \n",
        "\n",
        "\n",
        "    \n"
      ]
    }
  ]
}